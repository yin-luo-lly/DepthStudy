{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、竞赛介绍\n",
    "\n",
    " CCF大数据与计算智能大赛（CCF Big Data & Computing Intelligence Contest，简称CCF BDCI）由中国计算机学会于2013年创办。大赛由国家自然科学基金委员会指导，是大数据与人工智能领域的算法、应用和系统大型挑战赛事。大赛面向重点行业和应用领域征集需求，以前沿技术与行业应用问题为导向，以促进行业发展及产业升级为目标，以众智、众包的方式，汇聚海内外产学研用多方智慧，为社会发现和培养了大量高质量数据人才。\n",
    "  大赛迄今已成功举办八届，累计吸引全球1500余所高校、1800家企事业单位及80余所科研机构的12万余人参与，已成为中国大数据与人工智能领域最具影响力的活动之一，是中国大数据综合赛事第一品牌。\n",
    "  2021年第九届大赛以“数引创新，竞促汇智”为主题，立足余杭、面向全球，于9月至12月举办。大赛将致力于解决来自政府、企业真实场景中的痛点、难点问题，邀请全球优秀团队参与数据资源开发利用，广泛征集信息技术应用解决方案。\n",
    "  \n",
    "## 1.1  赛题任务\n",
    "\n",
    "比赛的地址为[https://www.datafountain.cn/competitions/518](https://www.datafountain.cn/competitions/518)\n",
    "\n",
    "本赛题提供一部分电影剧本作为训练集，训练集数据已由人工进行标注，参赛队伍需要对剧本场景中每句对白和动作描述中涉及到的每个角色的情感从多个维度进行分析和识别。该任务的主要难点和挑战包括：1）剧本的行文风格和通常的新闻类语料差别较大，更加口语化；2）剧本中角色情感不仅仅取决于当前的文本，对前文语义可能有深度依赖。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、多任务学习\n",
    "\n",
    "## 2.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:47.439316Z",
     "iopub.status.busy": "2024-12-19T13:12:47.438775Z",
     "iopub.status.idle": "2024-12-19T13:12:47.445782Z",
     "shell.execute_reply": "2024-12-19T13:12:47.444966Z",
     "shell.execute_reply.started": "2024-12-19T13:12:47.439283Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 导入paddle库\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from paddle.io import DataLoader\n",
    "from paddle.dataset.common import md5file\n",
    "# 导入paddlenlp的库\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from paddlenlp.transformers import BertTokenizer,BertPretrainedModel\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\n",
    "from paddlenlp.datasets import DatasetBuilder,get_path_from_url\n",
    "\n",
    "# 导入所需要的py包\n",
    "from paddle.io import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:47.447571Z",
     "iopub.status.busy": "2024-12-19T13:12:47.447248Z",
     "iopub.status.idle": "2024-12-19T13:12:47.891192Z",
     "shell.execute_reply": "2024-12-19T13:12:47.890116Z",
     "shell.execute_reply.started": "2024-12-19T13:12:47.447548Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data110628/剧本角色情感识别.zip\r\n",
      "  inflating: data/submit_example.tsv  \r\n",
      "  inflating: data/__MACOSX/._submit_example.tsv  \r\n",
      "  inflating: data/test_dataset.tsv   \r\n",
      "  inflating: data/__MACOSX/._test_dataset.tsv  \r\n",
      "  inflating: data/train_dataset_v2.tsv  \r\n",
      "  inflating: data/__MACOSX/._train_dataset_v2.tsv  \r\n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/data110628/剧本角色情感识别.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:47.893087Z",
     "iopub.status.busy": "2024-12-19T13:12:47.892588Z",
     "iopub.status.idle": "2024-12-19T13:12:48.107560Z",
     "shell.execute_reply": "2024-12-19T13:12:48.106702Z",
     "shell.execute_reply.started": "2024-12-19T13:12:47.893055Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42790/42790 [00:00<00:00, 799372.29it/s]\r\n"
     ]
    }
   ],
   "source": [
    "with open('data/train_dataset_v2.tsv', 'r', encoding='utf-8') as handler:\n",
    "    lines = handler.read().split('\\n')[1:-1]\n",
    "\n",
    "    data = list()\n",
    "    for line in tqdm(lines):\n",
    "        sp = line.split('\\t')\n",
    "        if len(sp) != 4:\n",
    "            print(\"ERROR:\", sp)\n",
    "            continue\n",
    "        data.append(sp)\n",
    "\n",
    "train = pd.DataFrame(data)\n",
    "train.columns = ['id', 'content', 'character', 'emotions']\n",
    "\n",
    "test = pd.read_csv('data/test_dataset.tsv', sep='\\t')\n",
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')\n",
    "train = train[train['emotions'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:48.109232Z",
     "iopub.status.busy": "2024-12-19T13:12:48.108708Z",
     "iopub.status.idle": "2024-12-19T13:12:48.758036Z",
     "shell.execute_reply": "2024-12-19T13:12:48.757140Z",
     "shell.execute_reply.started": "2024-12-19T13:12:48.109193Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train['text'] = train[ 'content'].astype(str)  +'角色: ' + train['character'].astype(str)\n",
    "test['text'] = test['content'].astype(str) + ' 角色: ' + test['character'].astype(str)\n",
    "\n",
    "train['emotions'] = train['emotions'].apply(lambda x: [int(_i) for _i in x.split(',')])\n",
    "\n",
    "train[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] = train['emotions'].values.tolist()\n",
    "test[['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']] =[0,0,0,0,0,0]\n",
    "\n",
    "train.to_csv('data/train.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)\n",
    "\n",
    "test.to_csv('data/test.csv',columns=['id', 'content', 'character','text','love', 'joy', 'fright', 'anger', 'fear', 'sorrow'],\n",
    "            sep='\\t',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 组装batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:48.760653Z",
     "iopub.status.busy": "2024-12-19T13:12:48.760092Z",
     "iopub.status.idle": "2024-12-19T13:12:55.220697Z",
     "shell.execute_reply": "2024-12-19T13:12:55.219756Z",
     "shell.execute_reply.started": "2024-12-19T13:12:48.760615Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-19 21:12:48,763] [    INFO] - Found /home/aistudio/.paddlenlp/models/roberta-wwm-ext/vocab.txt\r\n",
      "[2024-12-19 21:12:48,782] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/roberta-wwm-ext/roberta_chn_base.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "target_cols=['love', 'joy', 'fright', 'anger', 'fear', 'sorrow']\n",
    "# PRE_TRAINED_MODEL_NAME=\"bert-base-chinese\"\n",
    "# PRE_TRAINED_MODEL_NAME='macbert-base-chinese'\n",
    "\n",
    "# 读者可以在这里切换语言模型\n",
    "\n",
    "\n",
    "# 加载BERT的分词器\n",
    "# PRE_TRAINED_MODEL_NAME='macbert-large-chinese'\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# base_model=ppnlp.transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# PRE_TRAINED_MODEL_NAME='bert-wwm-ext-chinese'\n",
    "# base_model = ppnlp.transformers.BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# roberta\n",
    "PRE_TRAINED_MODEL_NAME='roberta-wwm-ext'\n",
    "tokenizer = ppnlp.transformers.RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "base_model = ppnlp.transformers.RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)  # 加载预训练模型\n",
    "# model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PaddleNLP更多预训练模型:**\n",
    "\n",
    "PaddleNLP不仅支持RoBERTa预训练模型，还支持ERNIE、BERT、RoBERTa、Electra等预训练模型。\n",
    "下表汇总了目前PaddleNLP支持的各类预训练模型。用户可以使用PaddleNLP提供的模型，完成问答、序列分类、token分类等任务。同时还提供了22种预训练的参数权重供用户使用，其中包含了11种中文语言模型的预训练权重。\n",
    "\n",
    "| Model | Tokenizer| Supported Task| Model Name|\n",
    "|---|---|---|---|\n",
    "| [BERT](https://arxiv.org/abs/1810.04805) | BertTokenizer|BertModel<br> BertForQuestionAnswering<br> BertForSequenceClassification<br>BertForTokenClassification| `bert-base-uncased`<br> `bert-large-uncased` <br>`bert-base-multilingual-uncased` <br>`bert-base-cased`<br> `bert-base-chinese`<br> `bert-base-multilingual-cased`<br> `bert-large-cased`<br> `bert-wwm-chinese`<br> `bert-wwm-ext-chinese` |\n",
    "|[ERNIE](https://arxiv.org/abs/1904.09223)|ErnieTokenizer<br>ErnieTinyTokenizer|ErnieModel<br> ErnieForQuestionAnswering<br> ErnieForSequenceClassification<br> ErnieForTokenClassification| `ernie-1.0`<br> `ernie-tiny`<br> `ernie-2.0-en`<br> `ernie-2.0-large-en`|\n",
    "|[RoBERTa](https://arxiv.org/abs/1907.11692)|RobertaTokenizer| RobertaModel<br>RobertaForQuestionAnswering<br>RobertaForSequenceClassification<br>RobertaForTokenClassification| `roberta-wwm-ext`<br> `roberta-wwm-ext-large`<br> `rbt3`<br> `rbtl3`|\n",
    "|[ELECTRA](https://arxiv.org/abs/2003.10555) |ElectraTokenizer| ElectraModel<br>ElectraForSequenceClassification<br>ElectraForTokenClassification<br>|`electra-small`<br> `electra-base`<br> `electra-large`<br> `chinese-electra-small`<br> `chinese-electra-base`<br>|\n",
    "\n",
    "注：其中中文的预训练模型有 `bert-base-chinese, bert-wwm-chinese, bert-wwm-ext-chinese, ernie-1.0, ernie-tiny, roberta-wwm-ext, roberta-wwm-ext-large, rbt3, rbtl3, chinese-electra-base, chinese-electra-small` 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.222667Z",
     "iopub.status.busy": "2024-12-19T13:12:55.221932Z",
     "iopub.status.idle": "2024-12-19T13:12:55.809689Z",
     "shell.execute_reply": "2024-12-19T13:12:55.808723Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.222639Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RoleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mode='train',trans_func=None):\n",
    "\n",
    "        super(RoleDataset, self).__init__()\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.data = pd.read_csv('data/train.csv',sep='\\t')\n",
    "        else:\n",
    "            self.data = pd.read_csv('data/test.csv',sep='\\t')\n",
    "        self.texts=self.data['text'].tolist()\n",
    "        self.labels=self.data[target_cols].to_dict('records')\n",
    "        self.trans_func=trans_func\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text=str(self.texts[index])\n",
    "        label=self.labels[index]\n",
    "        sample = {\n",
    "            'text': text\n",
    "        }\n",
    "        for label_col in target_cols:\n",
    "            sample[label_col] =label[label_col]/3.0\n",
    "        sample=self.trans_func(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.texts)\n",
    "\n",
    "# 转换成id的函数\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "    # print(example)\n",
    "    sample={}\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "    sample['input_ids'] = encoded_inputs[\"input_ids\"]\n",
    "    sample['token_type_ids'] = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    sample['love'] = np.array(example[\"love\"], dtype=\"float32\")\n",
    "    sample['joy'] = np.array(example[\"joy\"], dtype=\"float32\")\n",
    "    sample['anger'] = np.array(example[\"anger\"], dtype=\"float32\")\n",
    "\n",
    "    sample['fright'] = np.array(example[\"fright\"], dtype=\"float32\")\n",
    "    sample['fear'] = np.array(example[\"fear\"], dtype=\"float32\")\n",
    "    sample['sorrow'] = np.array(example[\"sorrow\"], dtype=\"float32\")\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "max_seq_length=128\n",
    "trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "train_ds=RoleDataset('train',trans_func)\n",
    "test_ds=RoleDataset('test',trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.811530Z",
     "iopub.status.busy": "2024-12-19T13:12:55.810776Z",
     "iopub.status.idle": "2024-12-19T13:12:55.817576Z",
     "shell.execute_reply": "2024-12-19T13:12:55.816821Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.811500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1921, 4958, 678, 4708, 3274, 7433, 8024, 157, 8144, 3633, 1762, 5314, 10905, 4959, 7433, 6132, 8024, 800, 5632, 2346, 1316, 1372, 4959, 4708, 1296, 5946, 4638, 1092, 6163, 8024, 2130, 1059, 3274, 7463, 1762, 1920, 7433, 722, 704, 511, 6235, 5682, 131, 157, 8144, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'love': array(0., dtype=float32), 'joy': array(0., dtype=float32), 'anger': array(0., dtype=float32), 'fright': array(0., dtype=float32), 'fear': array(0., dtype=float32), 'sorrow': array(0., dtype=float32)}\r\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.819298Z",
     "iopub.status.busy": "2024-12-19T13:12:55.818588Z",
     "iopub.status.idle": "2024-12-19T13:12:55.823413Z",
     "shell.execute_reply": "2024-12-19T13:12:55.822721Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.819269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=3\n",
    "weight_decay=0.0\n",
    "data_path='data'\n",
    "warmup_proportion=0.0\n",
    "init_from_ckpt=None\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "learning_rate=5e-5\n",
    "\n",
    "\n",
    "# # 把训练集合转换成id\n",
    "# train_ds = train_ds.map(partial(convert_example, tokenizer=tokenizer))\n",
    "\n",
    "# # 构建训练集合的dataloader\n",
    "# train_batch_sampler = paddle.io.BatchSampler(dataset=train_ds, batch_size=32, shuffle=True)\n",
    "# train_data_loader = paddle.io.DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.824933Z",
     "iopub.status.busy": "2024-12-19T13:12:55.824369Z",
     "iopub.status.idle": "2024-12-19T13:12:55.831989Z",
     "shell.execute_reply": "2024-12-19T13:12:55.831354Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.824903Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None):\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.833437Z",
     "iopub.status.busy": "2024-12-19T13:12:55.832881Z",
     "iopub.status.idle": "2024-12-19T13:12:55.841376Z",
     "shell.execute_reply": "2024-12-19T13:12:55.840648Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.833413Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate_func(batch_data):\n",
    "    # 获取batch数据的大小\n",
    "    batch_size = len(batch_data)\n",
    "    # 如果batch_size为0，则返回一个空字典\n",
    "    if batch_size == 0:\n",
    "        return {}\n",
    "    input_ids_list, attention_mask_list = [], []\n",
    "    love_list,joy_list,anger_list=[],[],[]\n",
    "    fright_list,fear_list,sorrow_list=[],[],[]\n",
    "    # 遍历batch数据，将每一个数据，转换成tensor的形式\n",
    "    for instance in batch_data:\n",
    "        input_ids_temp = instance[\"input_ids\"]\n",
    "        attention_mask_temp = instance[\"token_type_ids\"]\n",
    "\n",
    "        love=instance['love'] \n",
    "        joy=instance['joy'] \n",
    "        anger=instance['anger'] \n",
    "\n",
    "        fright= instance['fright'] \n",
    "        fear=instance['fear'] \n",
    "        sorrow=instance['sorrow'] \n",
    "\n",
    "        input_ids_list.append(paddle.to_tensor(input_ids_temp, dtype=\"int64\"))\n",
    "        attention_mask_list.append(paddle.to_tensor(attention_mask_temp, dtype=\"int64\"))\n",
    "\n",
    "        love_list.append(love)\n",
    "        joy_list.append(joy)\n",
    "        anger_list.append(anger)\n",
    "\n",
    "        fright_list.append(fright)\n",
    "        fear_list.append(fear)\n",
    "        sorrow_list.append(sorrow)\n",
    "\n",
    "    # 对一个batch内的数据，进行padding\n",
    "    return {\"input_ids\": Pad(pad_val=0, axis=0)(input_ids_list),\n",
    "            \"token_type_ids\": Pad(pad_val=0, axis=0)(attention_mask_list),\n",
    "            \"love\": Stack(dtype=\"int64\")(love_list),\n",
    "            \"joy\": Stack(dtype=\"int64\")(joy_list),\n",
    "            \"anger\": Stack(dtype=\"int64\")(anger_list),\n",
    "            \"fright\": Stack(dtype=\"int64\")(fright_list),\n",
    "            \"fear\": Stack(dtype=\"int64\")(fear_list),\n",
    "            \"sorrow\": Stack(dtype=\"int64\")(sorrow_list),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.842709Z",
     "iopub.status.busy": "2024-12-19T13:12:55.842233Z",
     "iopub.status.idle": "2024-12-19T13:12:55.866429Z",
     "shell.execute_reply": "2024-12-19T13:12:55.865707Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.842686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=collate_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.867642Z",
     "iopub.status.busy": "2024-12-19T13:12:55.867386Z",
     "iopub.status.idle": "2024-12-19T13:12:55.891193Z",
     "shell.execute_reply": "2024-12-19T13:12:55.890541Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.867620Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmotionClassifier(\r\n",
      "  (bert): RobertaModel(\r\n",
      "    (embeddings): RobertaEmbeddings(\r\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0, sparse=False)\r\n",
      "      (position_embeddings): Embedding(512, 768, sparse=False)\r\n",
      "      (token_type_embeddings): Embedding(2, 768, sparse=False)\r\n",
      "      (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "    )\r\n",
      "    (encoder): TransformerEncoder(\r\n",
      "      (layers): LayerList(\r\n",
      "        (0): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (1): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (2): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (3): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (4): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (5): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (6): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (7): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (8): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (9): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (10): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (11): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (pooler): RobertaPooler(\r\n",
      "      (dense): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "      (activation): Tanh()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (atten): Sequential(\r\n",
      "    (0): Linear(in_features=768, out_features=512, dtype=float32)\r\n",
      "    (1): Tanh()\r\n",
      "    (2): Linear(in_features=512, out_features=1, dtype=float32)\r\n",
      "  )\r\n",
      "  (out_love): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      "  (out_joy): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      "  (out_fright): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      "  (out_anger): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      "  (out_fear): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      "  (out_sorrow): Linear(in_features=768, out_features=1, dtype=float32)\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "weight_attr = paddle.framework.ParamAttr(\n",
    "    initializer=paddle.nn.initializer.XavierUniform())\n",
    "bias_attr = paddle.framework.ParamAttr(\n",
    "    initializer=paddle.nn.initializer.XavierUniform())\n",
    "\n",
    "soft = paddle.nn.Softmax(axis=1) \n",
    "\n",
    "class EmotionClassifier(nn.Layer):\n",
    "    def __init__(self, bert,n_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.atten = nn.Sequential(\n",
    "            nn.Linear(self.bert.config[\"hidden_size\"], 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1)\n",
    "        )  # 输出句子权重\n",
    "        self.out_love = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "        self.out_joy = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "        self.out_fright = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "        self.out_anger = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "        self.out_fear = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "        self.out_sorrow = nn.Linear(self.bert.config[\"hidden_size\"], n_classes, weight_attr=weight_attr, bias_attr=bias_attr)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "\n",
    "        # _, pooled_output_before = self.bert(\n",
    "        #     input_ids=input_ids,\n",
    "        #     token_type_ids=token_type_ids\n",
    "        # )\n",
    "        # dropout = paddle.nn.Dropout(p=0.1)\n",
    "        # pooled_output = dropout(pooled_output_before)\n",
    "        last_layer_hidden_states, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        weights1 = self.atten(last_layer_hidden_states)\n",
    "        weights = soft(weights1)\n",
    "        context_vector = paddle.sum(weights * last_layer_hidden_states, axis=1)\n",
    "        love = self.out_love(context_vector)\n",
    "        joy = self.out_joy(context_vector)\n",
    "        fright = self.out_fright(context_vector)\n",
    "        anger = self.out_anger(context_vector)\n",
    "        fear = self.out_fear(context_vector)\n",
    "        sorrow = self.out_sorrow(context_vector)\n",
    "        # love = self.out_love(weigh * pooled_output)\n",
    "        # joy = self.out_joy(pooled_output)\n",
    "        # fright = self.out_fright(pooled_output)\n",
    "        # anger = self.out_anger(pooled_output)\n",
    "        # fear = self.out_fear(pooled_output)\n",
    "        # sorrow = self.out_sorrow(pooled_output)\n",
    "        return {\n",
    "            'love': love, 'joy': joy, 'fright': fright,\n",
    "            'anger': anger, 'fear': fear, 'sorrow': sorrow,\n",
    "        }\n",
    "\n",
    "class_names=[1]\n",
    "model = EmotionClassifier(base_model,1)  #\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.892712Z",
     "iopub.status.busy": "2024-12-19T13:12:55.892047Z",
     "iopub.status.idle": "2024-12-19T13:12:55.899596Z",
     "shell.execute_reply": "2024-12-19T13:12:55.898916Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.892686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_train_epochs=3\n",
    "num_training_steps = len(train_data_loader) * num_train_epochs\n",
    "\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "# criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# criterion  = paddle.nn.MSELoss()\n",
    "criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T13:12:55.902888Z",
     "iopub.status.busy": "2024-12-19T13:12:55.902318Z",
     "iopub.status.idle": "2024-12-19T17:13:46.711603Z",
     "shell.execute_reply": "2024-12-19T17:13:46.709945Z",
     "shell.execute_reply.started": "2024-12-19T13:12:55.902862Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \r\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\r\n",
      "  if data.dtype == np.object:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 0, batch: 99, loss: 0.54279, accuracy: 0.98776, speed: 0.05 step/s\r\n",
      "global step 200, epoch: 0, batch: 199, loss: 0.35107, accuracy: 0.98661, speed: 0.02 step/s\r\n",
      "global step 300, epoch: 0, batch: 299, loss: 0.19149, accuracy: 0.98681, speed: 0.02 step/s\r\n",
      "global step 400, epoch: 0, batch: 399, loss: 0.18751, accuracy: 0.98706, speed: 0.01 step/s\r\n",
      "global step 500, epoch: 0, batch: 499, loss: 0.13485, accuracy: 0.98713, speed: 0.01 step/s\r\n",
      "global step 600, epoch: 0, batch: 599, loss: 0.56137, accuracy: 0.98688, speed: 0.01 step/s\r\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6605/3542091575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mdo_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6605/3542091575.py\u001b[0m in \u001b[0;36mdo_train\u001b[0;34m(model, data_loader, criterion, optimizer, scheduler, metric)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-238>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/wrapped_decorator.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py\u001b[0m in \u001b[0;36m__impl__\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         assert in_dygraph_mode(\n\u001b[1;32m    226\u001b[0m         ), \"We only support '%s()' in dynamic graph mode, please call 'paddle.disable_static()' to enter dynamic graph mode.\" % func.__name__\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__impl__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_tensor, retain_graph)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 core.dygraph_run_backward([self], [grad_tensor], retain_graph,\n\u001b[0;32m--> 239\u001b[0;31m                                           framework._dygraph_tracer())\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def do_train( model, data_loader,  criterion,  optimizer, scheduler,  metric ):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    log_steps = 100\n",
    "    train_losses = []  \n",
    "    train_accuracies = []  \n",
    "\n",
    "    shape_tensor = paddle.to_tensor(np.array([-1, 1]).astype(\"int32\"))  # 在这里定义shape_tensor\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        losses = []\n",
    "        for step, sample in enumerate(data_loader):\n",
    "            input_ids = sample[\"input_ids\"]\n",
    "            token_type_ids = sample[\"token_type_ids\"]\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids)\n",
    "\n",
    "            loss_love = criterion(outputs['love'], paddle.reshape(paddle.to_tensor(sample['love'], dtype='float32'),shape=shape_tensor))\n",
    "            loss_joy = criterion(outputs['joy'], paddle.reshape(paddle.to_tensor(sample['joy'], dtype='float32'),shape=shape_tensor))\n",
    "            loss_fright = criterion(outputs['fright'], paddle.reshape(paddle.to_tensor(sample['fright'], dtype='float32'),shape=shape_tensor))\n",
    "            loss_anger = criterion(outputs['anger'], paddle.reshape(paddle.to_tensor(sample['anger'], dtype='float32'),shape=shape_tensor))\n",
    "            loss_fear = criterion(outputs['fear'], paddle.reshape(paddle.to_tensor(sample['fear'], dtype='float32'),shape=shape_tensor))\n",
    "            loss_sorrow = criterion(outputs['sorrow'], paddle.reshape(paddle.to_tensor(sample['sorrow'], dtype='float32'),shape=shape_tensor))\n",
    "\n",
    "            loss = loss_love + loss_joy + loss_fright + loss_anger + loss_fear + loss_sorrow\n",
    "\n",
    "            for label_col in target_cols:\n",
    "                correct = metric.compute(outputs[label_col], sample[label_col])\n",
    "                metric.update(correct)\n",
    "\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            losses.append(loss.numpy())\n",
    "            train_losses.append(loss.numpy())\n",
    "            train_accuracies.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            global_step += 1\n",
    "            if global_step % log_steps == 0:\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, accuracy: %.5f, speed: %.2f step/s\"\n",
    "                      % (global_step, epoch, step, loss, acc,\n",
    "                         log_steps / (time.time() - tic_train)))\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "        metric.reset()\n",
    "\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_accuracies)\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return np.mean(losses)\n",
    "do_train(model,train_data_loader,criterion,optimizer,lr_scheduler,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T17:13:46.712485Z",
     "iopub.status.idle": "2024-12-19T17:13:46.712876Z",
     "shell.execute_reply": "2024-12-19T17:13:46.712708Z",
     "shell.execute_reply.started": "2024-12-19T17:13:46.712693Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "test_data_loader = create_dataloader(\n",
    "        test_ds,\n",
    "        mode='test',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=collate_func)\n",
    "        \n",
    "test_pred = defaultdict(list)\n",
    "m = nn.Sigmoid()\n",
    "for step, batch in tqdm(enumerate(test_data_loader)):\n",
    "    b_input_ids = batch['input_ids']\n",
    "    token_type_ids = batch['token_type_ids']\n",
    "    logits = model(input_ids=b_input_ids, token_type_ids=token_type_ids)\n",
    "    for col in target_cols:\n",
    "        out2 = paddle.argmax(logits[col], axis=1)\n",
    "        test_pred[col].append(out2.numpy())\n",
    "    print(test_pred)\n",
    "    # print(logits)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T17:13:46.714313Z",
     "iopub.status.idle": "2024-12-19T17:13:46.714852Z",
     "shell.execute_reply": "2024-12-19T17:13:46.714696Z",
     "shell.execute_reply.started": "2024-12-19T17:13:46.714681Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model,test_data_loader):\n",
    "    val_loss = 0\n",
    "    test_pred = defaultdict(list)\n",
    "    model.eval()\n",
    "    for step, batch in tqdm(enumerate(test_data_loader)):\n",
    "        b_input_ids = batch['input_ids']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "\n",
    "        with paddle.no_grad():\n",
    "            logits = model(input_ids=b_input_ids, token_type_ids=token_type_ids)\n",
    "            for col in target_cols:\n",
    "                # out2 = paddle.argmax(logits[col], axis=1)\n",
    "                # test_pred[col].extend(out2.numpy().tolist())\n",
    "                out2 = m(logits[col]).squeeze(1)*3.0 \n",
    "                test_pred[col].extend(out2.numpy().tolist())\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "submit = pd.read_csv('data/submit_example.tsv', sep='\\t')\n",
    "test_pred = predict(model,test_data_loader)\n",
    "\n",
    "# 绘制每个情感类别预测值的分布直方图\n",
    "for col in target_cols:\n",
    "    plt.hist(test_pred[col], bins=10)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{col} Prediction Distribution')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T17:13:46.715898Z",
     "iopub.status.idle": "2024-12-19T17:13:46.716204Z",
     "shell.execute_reply": "2024-12-19T17:13:46.716069Z",
     "shell.execute_reply.started": "2024-12-19T17:13:46.716057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_pred['love'][:10])\n",
    "print(len(test_pred['love']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 预测结果结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-19T17:13:46.717362Z",
     "iopub.status.idle": "2024-12-19T17:13:46.717683Z",
     "shell.execute_reply": "2024-12-19T17:13:46.717538Z",
     "shell.execute_reply.started": "2024-12-19T17:13:46.717525Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_preds = []\n",
    "for col in target_cols:\n",
    "    preds = test_pred[col]\n",
    "    label_preds.append(preds)\n",
    "print(len(label_preds[0]))\n",
    "sub = submit.copy()\n",
    "sub['emotion'] = np.stack(label_preds, axis=1).tolist()\n",
    "sub['emotion'] = sub['emotion'].apply(lambda x: ','.join([str(i) for i in x]))\n",
    "sub.to_csv('baseline_{}.tsv'.format(PRE_TRAINED_MODEL_NAME), sep='\\t', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型优化思路\n",
    "\n",
    "1.数据增强： [中文数据增强工具](https://github.com/425776024/nlpcda/)、回译等\n",
    "\n",
    "2.尝试不同的预训练模型、调参优化等。\n",
    "\n",
    "3.5fodls交叉验证、多模型结果融合等\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
